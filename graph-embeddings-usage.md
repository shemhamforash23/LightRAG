# Использование графовых эмбеддингов и поиск в LightRAG

## Проблема
В конфигурации LightRAG (`lightrag.py`) присутствуют параметры для настройки алгоритма встраивания узлов графа (node embeddings) с использованием `node2vec`:

```python
# lightrag/lightrag.py L155-178
# Node embedding
# ---

node_embedding_algorithm: str = field(default="node2vec")
"""Algorithm used for node embedding in knowledge graphs."""

node2vec_params: dict[str, int] = field(
    default_factory=lambda: {
        "dimensions": 1536,
        "num_walks": 10,
        "walk_length": 40,
        "window_size": 2,
        "iterations": 3,
        "random_seed": 3,
    }
)
"""Configuration for the node2vec embedding algorithm: ..."""
```

Однако, при анализе реализации хранилища Neo4j (`neo4j_impl.py`) выяснилось, что соответствующие методы либо не реализованы, либо не вызываются в процессе обработки запросов:

```python
# lightrag/kg/neo4j_impl.py L984-986
@with_semaphore
async def _node2vec_embed(self):
    print("Implemented but never called.")

# lightrag/kg/neo4j_impl.py L1400-1404
@with_semaphore
async def embed_nodes(
    self, algorithm: str
) -> tuple[np.ndarray[Any, Any], list[str]]:
    raise NotImplementedError
```

Возник вопрос: как LightRAG выполняет семантический поиск и извлекает информацию из графа знаний Neo4j в различных режимах (`local`, `global`, `hybrid`, `naive`, `mix`), если встроенные графовые эмбеддинги не используются?

## Механизм поиска

Анализ кода показал, что семантический поиск и извлечение информации в LightRAG при обработке запросов (`aquery`) опирается на **комбинацию внешних векторных баз данных и запросов к структуре графа Neo4j**, а не на встроенные графовые эмбеддинги (node2vec).

**Ключевые компоненты:**

1.  **Векторные базы данных (Vector DBs):**
    *   `entities_vdb`: Хранит эмбеддинги *текстовых описаний* сущностей (узлов графа). Используется для поиска релевантных сущностей по семантической близости к запросу.
    *   `relationships_vdb`: Хранит эмбеддинги *текстовых описаний* отношений (ребер графа). Используется для поиска релевантных отношений.
    *   `chunks_vdb`: Хранит эмбеддинги *текстовых чанков* исходных документов. Используется для прямого поиска релевантных фрагментов текста.
2.  **Графовая база данных (Neo4j - `knowledge_graph_inst`):**
    *   Используется для получения *структурной информации* и *полных атрибутов* узлов и отношений, которые были найдены на предыдущем шаге с помощью векторного поиска в `entities_vdb` и `relationships_vdb`.
    *   Позволяет находить связи между сущностями и связанные с ними текстовые чанки.
3.  **Хранилище текстовых чанков (`text_chunks_db`):**
    *   Key-Value хранилище для получения полного текста чанка по его ID.

**Процесс обработки запроса (упрощенно):**

1.  **Вызов `LightRAG.aquery` (`lightrag.py`)**: Основная точка входа для запросов.

    ```python
    # lightrag/lightrag.py
    async def aquery(
        self,
        query: str,
        param: QueryParam | None = None,
        conversation_history: list[dict[str, str]] | None = None,
    ) -> str | AsyncIterator[str]:
        # ... (parameter handling, cache check) ...

        # Default to query_with_keywords if no specific query function is defined
        query_func = query_with_keywords

        # ... (more setup) ...

        return await query_func(
            query,
            prompt, # constructed prompt
            param,
            self.chunk_entity_relation_graph, # BaseGraphStorage instance (Neo4jGraphStorage)
            self.entities_vdb,             # BaseVectorStorage instance
            self.relationships_vdb,        # BaseVectorStorage instance
            self.chunks_vdb,               # BaseVectorStorage instance
            self.text_chunks_db,           # BaseKVStorage instance
            self.global_config,
            hashing_kv=self.cache_kv,
        )
    ```

2.  **Передача в `query_with_keywords` (`operate.py`)**: Эта функция координирует дальнейшие действия.

3.  **Извлечение ключевых слов**: С помощью LLM из запроса извлекаются высокоуровневые (HL) и низкоуровневые (LL) ключевые слова (`extract_keywords_only`).

    ```python
    # lightrag/operate.py
    async def extract_keywords_only(...):
        # ... uses LLM to extract keywords ...
        return hl_keywords, ll_keywords
    ```

4.  **Диспетчеризация по режиму (`mode`) в `query_with_keywords`**: В зависимости от значения `param.mode` вызывается соответствующая функция.

    ```python
    # lightrag/operate.py
    async def query_with_keywords(...):
        # 1. Extract keywords
        hl_keywords, ll_keywords = await extract_keywords_only(...)
        param.hl_keywords = hl_keywords
        param.ll_keywords = ll_keywords

        formatted_question = f"{prompt}\n\n### Keywords:\n..." # Include keywords

        # 2. Dispatch based on mode
        if param.mode in ["local", "global", "hybrid"]:
            # Uses kg_query_with_keywords (which likely calls _build_query_context)
            return await kg_query_with_keywords(...)
        elif param.mode == "naive":
            # Pure vector search on chunks
            return await naive_query(...)
        elif param.mode == "mix":
            # Combines KG and vector search on chunks
            return await mix_kg_vector_query(...)
        else:
            raise ValueError(f"Unknown mode {param.mode}")
    ```

5.  **Выполнение поиска в зависимости от режима:**
    *   **`naive_query` (режим `naive`)**: Выполняется векторный поиск **только** по `chunks_vdb` для нахождения релевантных текстовых чанков. Граф (`knowledge_graph_inst`) **не используется**. Контекст формируется из найденных чанков.
    *   **`kg_query_with_keywords` (режимы `local`, `global`, `hybrid`)**: Использует `_build_query_context`.
        *   `_build_query_context` -> `_get_node_data`: Выполняет **векторный поиск** по `entities_vdb` -> получает ID релевантных узлов -> запрашивает полные данные узлов из Neo4j (`knowledge_graph_inst.get_node`) -> находит связанные текстовые чанки (`_find_most_related_text_unit_from_entities`).
        *   `_build_query_context` -> `_get_relation_data`: Выполняет **векторный поиск** по `relationships_vdb` -> получает ID релевантных отношений -> запрашивает полные данные отношений из Neo4j (`knowledge_graph_inst.get_edge`) -> находит связанные текстовые чанки (`_find_most_related_text_unit_from_relations`).
        *   Разница между `local`, `global`, `hybrid` заключается в том, как комбинируется информация из узлов, отношений и связанных чанков для формирования контекста LLM.
    *   **`mix_kg_vector_query` (режим `mix`)**: Комбинирует оба подхода.
        *   Вызывает `get_kg_context` (аналогично `kg_query_with_keywords`) для получения контекста из графа + векторных баз сущностей/отношений.
        *   Вызывает `get_vector_context` (аналогично `naive_query`) для получения контекста из векторной базы чанков (`chunks_vdb`).
        *   Объединяет оба контекста (`process_combine_contexts`) перед передачей в LLM.

## Выводы

*   **Семантический поиск выполняется с помощью внешних векторных баз данных** (`entities_vdb`, `relationships_vdb`, `chunks_vdb`), где хранятся эмбеддинги текстовых описаний сущностей, отношений и самих текстовых чанков.
*   **Neo4j (`knowledge_graph_inst`) используется как хранилище структурированных данных (граф)**. Его роль - предоставлять детали (атрибуты) узлов и отношений, а также информацию о связях между ними *после* того, как релевантные элементы идентифицированы через векторный поиск. Он также используется для связи сущностей/отношений с исходными текстовыми чанками (`text_chunks_db`).
*   **Встроенные эмбеддинги узлов Neo4j (node2vec) не используются** в текущей логике обработки RAG-запросов. Параметры `node2vec` в конфигурации, вероятно, являются заделом на будущее или для других задач.
*   **Разница между режимами (`local`, `global`, `hybrid`, `mix`, `naive`)** заключается в том, какие источники данных (векторные базы сущностей/отношений/чанков, граф Neo4j) используются и как их результаты комбинируются для формирования финального контекста для LLM.

## Детальный анализ комбинирования контекста

Проанализируем функции, отвечающие за сбор и комбинирование контекста в файле `lightrag/operate.py` и `lightrag/utils.py`.

### Ключевые функции

*   **`_build_query_context` (`operate.py`)**: Оркестратор для режимов `local`, `global`, `hybrid`.
    *   **`local`**: Вызывает `_get_node_data` с LL-ключами.
    *   **`global`**: Вызывает `_get_edge_data` с HL-ключами.
    *   **`hybrid`**: Асинхронно вызывает обе, затем объединяет результаты с помощью `combine_contexts`.
    *   Форматирует результат в CSV.
*   **`_get_node_data` (`operate.py`)**: Используется в `local`/`hybrid`.
    1.  Векторный поиск сущностей по LL-ключам (`entities_vdb`).
    2.  Получение данных сущностей и их степени из Neo4j.
    3.  Поиск связанных элементов:
        *   `_find_most_related_text_unit_from_entities`: Находит чанки по `source_id` сущностей.
        *   `_find_most_related_edges_from_entities`: Находит ребра, связанные с сущностями.
    4.  **Обрезка списка сущностей** по `max_token_for_local_context` (на основе `description`). *Важно: обрезка происходит *после* поиска связанных элементов.*
    5.  Форматирование найденных сущностей, *связанных* ребер и *связанных* чанков в CSV.
*   **`_get_edge_data` (`operate.py`)**: Используется в `global`/`hybrid`.
    1.  Векторный поиск отношений по HL-ключам (`relationships_vdb`).
    2.  Получение данных отношений и их степени из Neo4j.
    3.  Сортировка и **обрезка списка отношений** по `max_token_for_global_context` (на основе `description`).
    4.  Поиск связанных элементов:
        *   `_find_most_related_entities_from_relationships`: Находит сущности, связанные с отношениями.
        *   `_find_related_text_unit_from_relationships`: Находит чанки, связанные с отношениями.
    5.  Форматирование найденных отношений, *связанных* сущностей и *связанных* чанков в CSV.
*   **`_find_most_related_text_unit_from_entities` (`operate.py`)**:
    1.  Извлекает `source_id` (ID чанков) из данных сущностей.
    2.  Загружает чанки из `text_chunks_db`.
    3.  Обрезает список чанков по `max_token_for_text_unit`.
*   **`_find_related_text_unit_from_relationships` (`operate.py`)**:
    1.  Извлекает `source_id` (ID чанков) из данных отношений.
    2.  Загружает чанки из `text_chunks_db`.
    3.  Обрезает список чанков по `max_token_for_text_unit`.
*   **`_find_most_related_edges_from_entities` (`operate.py`)**:
    1.  Получает все ребра, связанные с входными сущностями, из Neo4j.
    2.  Получает данные ребер и их степень.
    3.  Сортирует и **обрезает список ребер** по `max_token_for_global_context`. *Примечание: используется лимит для global, даже при вызове из local/hybrid.*
*   **`combine_contexts` (`operate.py`)**: Используется в `hybrid`.
    *   Вызывает `process_combine_contexts` для объединения и дедупликации CSV-строк сущностей, отношений и чанков, полученных от `_get_node_data` и `_get_edge_data`.
*   **`mix_kg_vector_query` (`operate.py`)**: Режим `mix`.
    1.  Параллельно запускает `get_kg_context` и `get_vector_context`.
    2.  **`get_kg_context`**:
        *   Извлекает HL/LL ключи.
        *   Динамически определяет режим (`local`, `global`, `hybrid`) для `_build_query_context`.
        *   Возвращает результат `_build_query_context` (CSV-строки).
    3.  **`get_vector_context`**:
        *   Векторный поиск по `chunks_vdb` с `query + history`. Использует уменьшенный `top_k`.
        *   Загружает и обрезает чанки по `max_token_for_text_unit`.
        *   Форматирует чанки в одну строку.
    4.  **Комбинирование**: Использует `process_combine_contexts` для слияния *только чанков* из KG и векторного поиска. Формирует финальный промпт.
*   **`process_combine_contexts` (`utils.py`)**: Используется в `hybrid` и `mix`.
    1.  Разбирает две входные CSV-строки.
    2.  Извлекает заголовок.
    3.  **"Сплющивает" каждую строку данных**, объединяя все столбцы, кроме первого ID, через запятую.
    4.  Объединяет сплющенные строки и **удаляет дубликаты** на основе точного совпадения этих сплющенных строк.
    5.  Формирует новую CSV-строку с исходным заголовком и перенумерованными уникальными сплющенными строками.

### Недостатки текущего подхода

1.  **Потеря информации при слиянии (`process_combine_contexts`)**: "Сплющивание" строк перед дедупликацией грубо и ненадежно, теряет структуру данных.
2.  **Независимая обрезка**: Обрезка списков (сущностей, отношений, чанков) происходит *до* комбинирования (в `hybrid` и `mix` для KG части), что может привести к потере релевантной информации до ее рассмотрения в совокупности. Нет общего ранжирования комбинированного контекста.
3.  **Неконсистентные лимиты обрезки**: `_find_most_related_edges_from_entities` всегда использует `max_token_for_global_context`, даже когда вызывается в контексте `local`/`hybrid`.
4.  **Отсутствие сквозного скоринга**: Скоры релевантности изначальных поисков не используются явно при комбинировании и финальной обрезке.
5.  **Простое объединение в `mix`**: Контексты KG и векторного поиска чанков в режиме `mix` комбинируются довольно поверхностно (сливаются только строки чанков).

### Потенциальные направления для улучшения

1.  **Улучшить логику слияния**: Заменить `process_combine_contexts` на слияние списков объектов *до* форматирования. Дедуплицировать по уникальным ID, при дублировании выбирать лучший скор или объединять метаданные.
2.  **Комбинировать *перед* обрезкой**: Сначала объединять списки данных из разных источников, затем применять единое ранжирование и обрезку к *общему* списку по общему лимиту токенов.
3.  **Использовать скоры релевантности**: Сохранять и использовать скоры изначальных поисков на всех этапах.
4.  **Согласовать лимиты обрезки**: Передавать корректный лимит токенов в функции поиска связанных элементов.
5.  **Интеграция в `mix`**: Рассмотреть более глубокую интеграцию KG и векторного поиска в режиме `mix`.
6.  **Эксперименты с форматом**: Попробовать JSON или другие структурированные форматы вместо CSV для LLM.
7.  **Оптимизация запросов**: Использовать полный запрос вместо разделения на HL/LL ключевые слова.
8.  **Глубина поиска**: Исследовать возможность многошагового поиска по графу.

---

**Ключевой вывод остается прежним:** встроенные графовые эмбеддинги (Node2Vec/HashGNN) **не задействованы** в RAG-пайплайне. Поиск основан на **внешних векторных базах данных** (для семантики текста) и **структурных запросах к Neo4j** (для деталей и связей).

Анализ комбинирования контекста выявил несколько **реальных узких мест и потенциальных областей для улучшения** в текущей архитектуре, которые гораздо важнее, чем замена неиспользуемого Node2Vec на HashGNN.

**Обновленные и Уточненные Рекомендации:**

Учитывая, что графовые эмбеддинги GDS не используются, и основываясь на вашем анализе сборки контекста, вот пересмотренный список рекомендаций, сфокусированный на улучшении *существующей* архитектуры:

1.  **Улучшение Структуры Графа (Высокий Приоритет):**
    *   **Задача:** Внедрить **семантически значимые типы связей** вместо общего `:DIRECTED`.
    *   **Обоснование:** Это напрямую улучшит качество структурной информации, получаемой из Neo4j *после* векторного поиска. Позволит писать более точные запросы для `_find_most_related_edges_from_entities` и `_find_most_related_entities_from_relationships`, а также сделает граф более интерпретируемым. LLM сможет лучше понять контекст, если связи будут типизированы.
    *   **Действия:**
        *   Определить семантические типы связей.
        *   Модифицировать логику, вызывающую `upsert_edge`, для определения и передачи нужного типа.
        *   Обновить `upsert_edge`, используя `apoc.merge.relationship` для динамической вставки типизированных связей.

2.  **Рефакторинг Комбинирования и Обрезки Контекста (Высокий Приоритет):**
    *   **Задача:** Исправить потерю информации и неоптимальную обрезку при сборке контекста, особенно в режимах `hybrid` и `mix`.
    *   **Обоснование:** Текущая логика (`process_combine_contexts`, независимая обрезка) теряет детали и может отбрасывать релевантную информацию преждевременно.
    *   **Действия:**
        *   **Переработать `process_combine_contexts`:** Вместо "сплющивания" строк, функция должна работать со списками объектов (сущностей, отношений, чанков). Дедупликацию проводить по уникальным ID (`entity_id`, `source_id` для чанков, ID ребер), сохраняя при этом полные данные и, возможно, выбирая объект с лучшим скором релевантности при дублировании.
        *   **Изменить порядок:** Сначала **собирать *всех* кандидатов** (сущности, отношения, чанки) из всех релевантных источников (векторный поиск, связанные элементы Neo4j) в единый пул *вместе со скорами релевантности* (если доступны). Затем **ранжировать этот общий пул** (например, по скорам, степени узла/ребра, или более сложной логике) и **обрезать его** до достижения общего лимита токенов (`max_tokens` для LLM).
        *   **Использовать скоры:** Пробрасывать скоры изначальных векторных поисков (`entities_vdb`, `relationships_vdb`, `chunks_vdb`) и использовать их в общем ранжировании перед обрезкой.
        *   **Согласовать лимиты:** Убедиться, что функции вроде `_find_most_related_edges_from_entities` используют лимиты, соответствующие контексту вызова (`local` или `global`).

3.  **Оптимизация Векторного Поиска (Средний Приоритет):**
    *   **Задача:** Улучшить качество первоначального поиска кандидатов.
    *   **Обоснование:** Качество всего RAG-процесса сильно зависит от того, насколько релевантны элементы, найденные на первом этапе векторного поиска.
    *   **Действия:**
        *   **Оценить модель эмбеддингов:** `text-embedding-3-large` - мощная модель, но стоит убедиться, что она оптимальна для ваших данных. Оценить качество получаемых эмбеддингов для сущностей, отношений и чанков.
        *   **Эксперимент с запросом:** Вместо извлечения ключевых слов (`extract_keywords_only`), попробовать использовать **эмбеддинг полного запроса пользователя** (возможно, с историей диалога) для поиска в `entities_vdb`, `relationships_vdb`, `chunks_vdb`. Это может дать более точные семантические совпадения, чем поиск по отдельным ключевым словам.
        *   **Качество описаний:** Убедиться, что текстовые описания сущностей и отношений, которые эмбеддируются и сохраняются в `entities_vdb` и `relationships_vdb`, являются качественными и репрезентативными.

4.  **Исследование Многошагового Поиска по Графу (Средний/Низкий Приоритет):**
    *   **Задача:** Расширить контекст за пределы непосредственно связанных элементов.
    *   **Обоснование:** Текущий поиск по графу в основном находит элементы, напрямую связанные с результатами векторного поиска (1 шаг). Возможно, релевантная информация находится на расстоянии 2-3 "прыжков" по графу.
    *   **Действия:**
        *   После нахождения начальных сущностей/отношений, добавить шаги для исследования их соседей второго или третьего порядка в Neo4j (с использованием уже типизированных связей!).
        *   Разработать стратегию ранжирования и отбора узлов/ребер, найденных на этих дополнительных шагах, чтобы не переполнить контекст.

5.  **Интеграция Структурных Графовых Эмбеддингов (Опционально, Низкий Приоритет):**
    *   **Задача:** Добавить дополнительный сигнал структурной близости.
    *   **Обоснование:** Может помочь найти структурно похожие узлы, упущенные текстовым поиском.
    *   **Действия:**
        *   Если решено внедрять: выбрать HashGNN, подготовить `featureProperties` (например, `descriptionEmbedding` *в Neo4j*), вычислить эмбеддинги.
        *   **Спроектировать интеграцию:** Определить, как использовать эти структурные эмбеддинги. Например:
            *   **Расширение:** После текстового поиска найти N ближайших соседей по графовому эмбеддингу в Neo4j.
            *   **Переранжирование:** Использовать близость графовых эмбеддингов как один из факторов в общем ранжировании перед обрезкой контекста.
        *   Реализовать эту интеграцию в функциях сборки контекста (`operate.py`).

**Приоритеты:**

1.  **Внедрение семантических типов связей.** (Улучшает используемую часть Neo4j).
2.  **Рефакторинг сборки и обрезки контекста.** (Исправляет потерю информации и улучшает релевантность для LLM).
3.  **Оптимизация векторного поиска** (качество эмбеддингов, метод запроса).
4.  **Исследование многошагового поиска по графу.**
5.  **Рассмотрение интеграции структурных графовых эмбеддингов.**

Этот план фокусируется на улучшении *существующей и реально используемой* архитектуры вашей системы LightRAG.
